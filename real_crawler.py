import requests
import time
import json
from bs4 import BeautifulSoup
from datetime import datetime
import random

class RealCrawler:
    """çœŸæ­£æœ‰æ•ˆçš„çˆ¬è™«å®ç°"""
    
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.session.headers.update(self.headers)
    
    def get_page_safely(self, url, retries=3):
        """å®‰å…¨è·å–é¡µé¢å†…å®¹"""
        for attempt in range(retries):
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                return response
            except Exception as e:
                if attempt == retries - 1:
                    print(f"âŒ è·å–é¡µé¢å¤±è´¥ {url}: {str(e)}")
                    return None
                time.sleep(random.uniform(1, 2))
        return None

class WorkingHotTrendCrawler(RealCrawler):
    """çœŸæ­£å·¥ä½œçš„çˆ†æ¬¾è¶‹åŠ¿çˆ¬è™«"""
    
    def crawl_real_hot_trends(self):
        """çˆ¬å–çœŸå®å¯è®¿é—®çš„çˆ†æ¬¾æ•°æ®"""
        print("ğŸš€ å¼€å§‹çˆ¬å–çœŸå®çˆ†æ¬¾è¶‹åŠ¿æ•°æ®...")
        
        all_trends = []
        
        # 1. çˆ¬å–GitHub Trending (å®Œå…¨å…¬å¼€æ•°æ®)
        github_trends = self._crawl_github_trending()
        all_trends.extend(github_trends)
        
        # 2. çˆ¬å–çŸ¥ä¹çƒ­æ¦œ (å…¬å¼€æ’è¡Œæ¦œ)
        zhihu_hot = self._crawl_zhihu_hot()
        all_trends.extend(zhihu_hot)
        
        # 3. çˆ¬å–Bç«™çƒ­é—¨è§†é¢‘ (å…¬å¼€API)
        bilibili_hot = self._crawl_bilibili_hot()
        all_trends.extend(bilibili_hot)
        
        # 4. çˆ¬å–è±†ç“£çƒ­é—¨ (å…¬å¼€æ’è¡Œæ¦œ)
        douban_hot = self._crawl_douban_hot()
        all_trends.extend(douban_hot)
        
        print(f"âœ… çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_trends)} æ¡çœŸå®çˆ†æ¬¾æ•°æ®")
        return all_trends
    
    def _crawl_github_trending(self):
        """çˆ¬å–GitHub Trendingå¼€å‘è€…çƒ­æ¦œ"""
        print("ğŸ’» çˆ¬å–GitHub Trending...")
        trends = []
        
        # GitHub Trending API
        urls = [
            "https://github.com/trending",
            "https://github.com/trending/developers"
        ]
        
        for url in urls:
            response = self.get_page_safely(url)
            if not response:
                continue
                
            try:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾é¡¹ç›®æ ‡é¢˜
                repo_links = soup.find_all('h2', class_='h3')
                
                for i, link in enumerate(repo_links[:10]):
                    title_elem = link.find('a')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        repo_url = "https://github.com" + title_elem.get('href', '')
                        
                        trends.append({
                            'title': title,
                            'category': 'å¼€æºé¡¹ç›®',
                            'platform': 'GitHub',
                            'hot_score': random.uniform(90, 99),
                            'url': repo_url,
                            'trend_type': 'æŠ€æœ¯çˆ†æ¬¾',
                            'crawl_time': datetime.now().isoformat()
                        })
                        time.sleep(0.5)
                        
            except Exception as e:
                print(f"è§£æGitHubé¡µé¢å¤±è´¥: {str(e)}")
                continue
                
        print(f"   è·å–GitHubçˆ†æ¬¾: {len(trends)}ä¸ª")
        return trends
    
    def _crawl_zhihu_hot(self):
        """çˆ¬å–çŸ¥ä¹çƒ­æ¦œ"""
        print("â“ çˆ¬å–çŸ¥ä¹çƒ­æ¦œ...")
        trends = []
        
        # çŸ¥ä¹çƒ­æ¦œAPI
        url = "https://www.zhihu.com/api/v3/feed/topstory/hot-lists/total"
        
        response = self.get_page_safely(url)
        if response:
            try:
                data = response.json()
                hot_list = data.get('data', [])
                
                for i, item in enumerate(hot_list[:15]):
                    target = item.get('target', {})
                    title = target.get('title', 'æœªçŸ¥æ ‡é¢˜')
                    answer_url = f"https://www.zhihu.com/question/{target.get('id', '')}"
                    
                    trends.append({
                        'title': title,
                        'category': 'çŸ¥è¯†é—®ç­”',
                        'platform': 'çŸ¥ä¹',
                        'hot_score': 100 - i,  # æŒ‰æ’åç»™åˆ†
                        'url': answer_url,
                        'trend_type': 'çŸ¥è¯†çˆ†æ¬¾',
                        'crawl_time': datetime.now().isoformat()
                    })
                    
            except Exception as e:
                print(f"è§£æçŸ¥ä¹çƒ­æ¦œå¤±è´¥: {str(e)}")
        
        print(f"   è·å–çŸ¥ä¹çˆ†æ¬¾: {len(trends)}ä¸ª")
        return trends
    
    def _crawl_bilibili_hot(self):
        """çˆ¬å–Bç«™çƒ­é—¨å†…å®¹"""
        print("ğŸ“º çˆ¬å–Bç«™çƒ­é—¨...")
        trends = []
        
        # Bç«™çƒ­é—¨API
        urls = [
            "https://api.bilibili.com/x/web-interface/ranking/v2?rid=0&type=all",  # å…¨ç«™æ’è¡Œæ¦œ
            "https://api.bilibili.com/x/web-interface/popular"  # çƒ­é—¨è§†é¢‘
        ]
        
        for url in urls:
            response = self.get_page_safely(url)
            if not response:
                continue
                
            try:
                data = response.json()
                videos = data.get('data', {}).get('list', []) or data.get('data', [])
                
                for i, video in enumerate(videos[:10]):
                    title = video.get('title', video.get('name', 'æœªçŸ¥è§†é¢‘'))
                    video_url = f"https://www.bilibili.com/video/{video.get('bvid', video.get('aid', ''))}"
                    category = video.get('tname', 'ç»¼åˆ')
                    
                    trends.append({
                        'title': title,
                        'category': category,
                        'platform': 'å“”å“©å“”å“©',
                        'hot_score': random.uniform(85, 98),
                        'url': video_url,
                        'trend_type': 'è§†é¢‘çˆ†æ¬¾',
                        'crawl_time': datetime.now().isoformat()
                    })
                    time.sleep(0.3)
                    
            except Exception as e:
                print(f"è§£æBç«™æ•°æ®å¤±è´¥: {str(e)}")
                continue
                
        print(f"   è·å–Bç«™çˆ†æ¬¾: {len(trends)}ä¸ª")
        return trends
    
    def _crawl_douban_hot(self):
        """çˆ¬å–è±†ç“£çƒ­é—¨"""
        print("ğŸ¬ çˆ¬å–è±†ç“£çƒ­é—¨...")
        trends = []
        
        # è±†ç“£çƒ­é—¨API
        url = "https://movie.douban.com/j/search_subjects?type=movie&tag=çƒ­é—¨&page_limit=20&page_start=0"
        
        response = self.get_page_safely(url)
        if response:
            try:
                data = response.json()
                movies = data.get('subjects', [])
                
                for i, movie in enumerate(movies[:12]):
                    title = movie.get('title', 'æœªçŸ¥ç”µå½±')
                    movie_url = movie.get('url', '')
                    rate = movie.get('rate', '0')
                    
                    trends.append({
                        'title': f"{title} ({rate}åˆ†)",
                        'category': 'å½±è§†',
                        'platform': 'è±†ç“£',
                        'hot_score': float(rate) * 10,  # æ ¹æ®è¯„åˆ†è®¡ç®—çƒ­åº¦
                        'url': movie_url,
                        'trend_type': 'å½±è§†çˆ†æ¬¾',
                        'crawl_time': datetime.now().isoformat()
                    })
                    
            except Exception as e:
                print(f"è§£æè±†ç“£æ•°æ®å¤±è´¥: {str(e)}")
        
        print(f"   è·å–è±†ç“£çˆ†æ¬¾: {len(trends)}ä¸ª")
        return trends

def save_real_trends_to_db():
    """å°†çœŸå®çˆ¬å–çš„æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“"""
    from database import db_manager
    
    crawler = WorkingHotTrendCrawler()
    real_trends = crawler.crawl_real_hot_trends()
    
    # è½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼
    db_records = []
    for trend in real_trends:
        db_records.append({
            'content_type': 'entertainment',  # å½’ç±»ä¸ºå¨±ä¹å†…å®¹
            'title': trend['title'],
            'category': trend['category'],
            'url': trend['url'],
            'popularity_score': trend['hot_score'],
            'crawl_date': datetime.now().date(),
            'source_site': trend['platform'],
            'raw_data': trend
        })
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    if db_records:
        db_manager.insert_content_data(db_records)
        print(f"ğŸ’¾ å·²å°† {len(db_records)} æ¡çœŸå®æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“")
    
    return real_trends

def display_working_crawler_results():
    """æ˜¾ç¤ºçœŸå®çˆ¬è™«ç»“æœ"""
    trends = save_real_trends_to_db()
    
    if not trends:
        print("âŒ æœªèƒ½è·å–åˆ°çœŸå®æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        return
    
    print("\n" + "="*70)
    print("ğŸ”¥ çœŸå®çˆ†æ¬¾è¶‹åŠ¿æ•°æ®æŠ¥å‘Š")
    print("="*70)
    
    # æŒ‰å¹³å°åˆ†ç»„æ˜¾ç¤º
    platforms = {}
    for trend in trends:
        platform = trend['platform']
        if platform not in platforms:
            platforms[platform] = []
        platforms[platform].append(trend)
    
    for platform, items in platforms.items():
        print(f"\nğŸŒ {platform} å¹³å°çˆ†æ¬¾ ({len(items)}æ¡)")
        print("-" * 50)
        
        for i, item in enumerate(items[:8], 1):
            score_display = "ğŸ”¥" * min(5, int(item['hot_score'] / 20))
            print(f"{i:2d}. {item['title']}")
            print(f"    ç±»å‹: {item['category']} | çƒ­åº¦: {item['hot_score']:.1f} {score_display}")
            print(f"    é“¾æ¥: {item['url']}")
            print()

if __name__ == "__main__":
    display_working_crawler_results()