import requests
import time
import json
import re
from bs4 import BeautifulSoup
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin, urlparse
import whois
from datetime import datetime
import psutil
import GPUtil

class SmartSearchEngine:
    """æ™ºèƒ½æœç´¢å¼•æ“ - ä¸“é—¨é’ˆå¯¹æ¼«å‰§è¡Œä¸š"""
    
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        self.session.headers.update(self.headers)
        
        # æ¼«å‰§è¡Œä¸šå…³é”®è¯åº“
        self.manga_keywords = [
            'æ¼«ç”»', 'åŠ¨æ¼«', 'æ¼«å‰§', 'äºŒæ¬¡å…ƒ', 'ACG', 'è¿è½½', 'å®Œç»“',
            'çƒ­é—¨æ¼«ç”»', 'æ–°ç•ª', 'äººæ°”', 'æ’è¡Œæ¦œ', 'æ¨è',
            'comic', 'manga', 'anime', 'manhua', 'manhwa'
        ]
        
        # ç›®æ ‡ç½‘ç«™æ± 
        self.target_sites = [
            'bilibili.com', 'kuaikanmanhua.com', 'manhuatai.com',
            'qq.com', '163.com', 'sina.com.cn', 'sohu.com',
            'douban.com', 'zhihu.com', 'weibo.com'
        ]
    
    def search_with_engines(self, query, max_results=50):
        """å¤šæœç´¢å¼•æ“æŸ¥è¯¢"""
        print(f"ğŸ” æœç´¢æŸ¥è¯¢: {query}")
        
        all_results = []
        
        # æ„é€ è¡Œä¸šç›¸å…³æŸ¥è¯¢
        industry_queries = [
            query,
            f"{query} æ¼«ç”»",
            f"{query} åŠ¨æ¼«",
            f"{query} çƒ­é—¨",
            f"{query} æ’è¡Œæ¦œ"
        ]
        
        for search_query in industry_queries:
            # æ¨¡æ‹Ÿæœç´¢å¼•æ“ç»“æœï¼ˆå®é™…åº”æ¥å…¥çœŸå®APIï¼‰
            engine_results = self._simulate_search_results(search_query)
            all_results.extend(engine_results)
            time.sleep(1)
        
        # å»é‡å’Œæ’åº
        unique_results = self._deduplicate_results(all_results)
        return sorted(unique_results, key=lambda x: x['relevance'], reverse=True)[:max_results]
    
    def _simulate_search_results(self, query):
        """æ¨¡æ‹Ÿæœç´¢å¼•æ“ç»“æœï¼ˆå®é™…åº”æ›¿æ¢ä¸ºçœŸå®APIï¼‰"""
        # è¿™é‡Œåº”è¯¥æ¥å…¥çœŸå®çš„æœç´¢å¼•æ“API
        # ç°åœ¨æ¨¡æ‹Ÿä¸€äº›æ¼«å‰§ç›¸å…³å†…å®¹
        base_urls = [
            "https://www.bilibili.com/read/cv",
            "https://www.kuaikanmanhua.com/web/topic",
            "https://manhua.dmzj.com/info",
            "https://www.manhuatai.com"
        ]
        
        results = []
        for i in range(5):
            url = f"{base_urls[i % len(base_urls)]}/{i+1000}"
            title = f"{query} çƒ­é—¨ä½œå“ç¬¬{i+1}å"
            snippet = f"è¿™æ˜¯å…³äº{query}çš„çƒ­é—¨æ¼«å‰§ä½œå“ï¼Œå—åˆ°äº†å¹¿æ³›å…³æ³¨..."
            
            results.append({
                'title': title,
                'url': url,
                'snippet': snippet,
                'relevance': 100 - i * 5,
                'source': 'simulated'
            })
        
        return results
    
    def _deduplicate_results(self, results):
        """ç»“æœå»é‡"""
        seen_urls = set()
        unique_results = []
        
        for result in results:
            if result['url'] not in seen_urls:
                seen_urls.add(result['url'])
                unique_results.append(result)
        
        return unique_results

class ProtocolChecker:
    """çˆ¬è™«åè®®æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.robot_parsers = {}
    
    def check_robots_txt(self, url):
        """æ£€æŸ¥robots.txtåè®®"""
        try:
            parsed_url = urlparse(url)
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            if base_url not in self.robot_parsers:
                rp = RobotFileParser()
                robots_url = urljoin(base_url, '/robots.txt')
                rp.set_url(robots_url)
                rp.read()
                self.robot_parsers[base_url] = rp
            
            # æ£€æŸ¥æ˜¯å¦å…è®¸çˆ¬å–
            can_fetch = self.robot_parsers[base_url].can_fetch('*', url)
            return {
                'allowed': can_fetch,
                'checked_url': url,
                'robots_url': urljoin(base_url, '/robots.txt')
            }
            
        except Exception as e:
            return {
                'allowed': False,
                'error': str(e),
                'checked_url': url
            }
    
    def check_site_legality(self, url):
        """æ£€æŸ¥ç½‘ç«™åˆæ³•æ€§"""
        try:
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
            
            # ç®€å•çš„åŸŸåæ£€æŸ¥
            whois_info = whois.whois(domain)
            return {
                'domain': domain,
                'registered': whois_info.registrar is not None,
                'creation_date': str(whois_info.creation_date) if whois_info.creation_date else 'Unknown'
            }
        except Exception as e:
            return {
                'domain': urlparse(url).netloc,
                'error': str(e)
            }

class MangaIndustryCrawler(SmartSearchEngine, ProtocolChecker):
    """æ¼«å‰§è¡Œä¸šä¸“ä¸šçˆ¬è™«"""
    
    def __init__(self):
        SmartSearchEngine.__init__(self)
        ProtocolChecker.__init__(self)
        self.crawled_domains = set()
        self.industry_patterns = {
            'manga_title': r'[\u4e00-\u9fff\w\s\-_\(\)]+(æ¼«ç”»|åŠ¨æ¼«|æ¼«å‰§)',
            'popularity_indicator': r'(çƒ­é—¨|äººæ°”|ç«çˆ†| trending |hot |popular )',
            'rating_pattern': r'(\d+\.\d+åˆ†|\d+ä¸‡äººæ°”|\d+ä¸‡ç‚¹å‡»)'
        }
    
    def discover_industry_targets(self, industry_term="æ¼«å‰§"):
        """å‘ç°è¡Œä¸šç›¸å…³ç›®æ ‡ç½‘ç«™"""
        print(f"ğŸŒ å‘ç°{industry_term}ç›¸å…³ç›®æ ‡...")
        
        # æœç´¢è¡Œä¸šç›¸å…³ç½‘ç«™
        search_results = self.search_with_engines(industry_term)
        
        valid_targets = []
        for result in search_results:
            url = result['url']
            protocol_check = self.check_robots_txt(url)
            legality_check = self.check_site_legality(url)
            
            if protocol_check['allowed'] and legality_check.get('registered', False):
                valid_targets.append({
                    'url': url,
                    'title': result['title'],
                    'relevance': result['relevance'],
                    'protocol_ok': True,
                    'legal': True
                })
                print(f"   âœ… {url} - ç¬¦åˆçˆ¬å–æ¡ä»¶")
            else:
                print(f"   âŒ {url} - ä¸ç¬¦åˆçˆ¬å–æ¡ä»¶")
        
        return valid_targets
    
    def crawl_manga_content(self, targets):
        """çˆ¬å–æ¼«å‰§å†…å®¹"""
        print("ğŸ•·ï¸ å¼€å§‹çˆ¬å–æ¼«å‰§å†…å®¹...")
        manga_data = []
        
        for target in targets[:10]:  # é™åˆ¶çˆ¬å–æ•°é‡
            try:
                print(f"   çˆ¬å–: {target['url']}")
                response = self.session.get(target['url'], timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æå–æ¼«å‰§ç›¸å…³ä¿¡æ¯
                manga_info = self._extract_manga_info(soup, target['url'])
                if manga_info:
                    manga_info['source_url'] = target['url']
                    manga_info['crawl_time'] = datetime.now().isoformat()
                    manga_data.append(manga_info)
                
                time.sleep(2)  # é¿å…è¿‡äºé¢‘ç¹
                
            except Exception as e:
                print(f"   âŒ çˆ¬å–å¤±è´¥ {target['url']}: {str(e)}")
                continue
        
        return manga_data
    
    def _extract_manga_info(self, soup, url):
        """æå–æ¼«å‰§ä¿¡æ¯"""
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            title_selectors = [
                'h1', 'h2', '.title', '.comic-title',
                '[class*="title"]', '[id*="title"]'
            ]
            
            title = None
            for selector in title_selectors:
                elem = soup.select_one(selector)
                if elem:
                    title_text = elem.get_text(strip=True)
                    if len(title_text) > 3 and len(title_text) < 100:
                        title = title_text
                        break
            
            if not title:
                return None
            
            # æå–å…¶ä»–ä¿¡æ¯
            manga_info = {
                'title': title,
                'category': self._classify_manga_category(title),
                'popularity_score': self._estimate_popularity(soup),
                'platform': urlparse(url).netloc,
                'url': url
            }
            
            return manga_info
            
        except Exception as e:
            return None
    
    def _classify_manga_category(self, title):
        """åˆ†ç±»æ¼«å‰§ç±»å‹"""
        categories = {
            'æ‹çˆ±': ['æ‹çˆ±', 'æµªæ¼«', 'çˆ±æƒ…', 'æ‹çˆ±å–œå‰§'],
            'æ ¡å›­': ['æ ¡å›­', 'å­¦å›­', 'å­¦ç”Ÿ', 'é’æ˜¥'],
            'å¥‡å¹»': ['å¥‡å¹»', 'é­”æ³•', 'å¼‚ä¸–ç•Œ', 'ç„å¹»'],
            'æç¬‘': ['æç¬‘', 'å–œå‰§', 'å¹½é»˜', 'æ¬¢ä¹'],
            'çƒ­è¡€': ['çƒ­è¡€', 'æˆ˜æ–—', 'å†’é™©', 'åŠ¨ä½œ'],
            'æ²»æ„ˆ': ['æ²»æ„ˆ', 'æ¸©é¦¨', 'æ—¥å¸¸', 'ç”Ÿæ´»']
        }
        
        for category, keywords in categories.items():
            if any(keyword in title for keyword in keywords):
                return category
        
        return 'å…¶ä»–'
    
    def _estimate_popularity(self, soup):
        """ä¼°ç®—çƒ­åº¦åˆ†æ•°"""
        # ç®€å•çš„çƒ­åº¦ä¼°ç®—é€»è¾‘
        text_content = soup.get_text()
        score = 50  # åŸºç¡€åˆ†æ•°
        
        # æ ¹æ®å…³é”®è¯åŠ åˆ†
        popularity_keywords = ['çƒ­é—¨', 'äººæ°”', 'ç«çˆ†', 'æ¨è', 'å¿…çœ‹']
        for keyword in popularity_keywords:
            if keyword in text_content:
                score += 5
        
        # æ ¹æ®æ•°å­—ä¿¡æ¯åŠ åˆ†
        numbers = re.findall(r'\d+', text_content)
        if numbers:
            avg_number = sum(int(n) for n in numbers[:10]) / len(numbers[:10])
            score += min(avg_number / 1000, 20)
        
        return min(score, 100)

class HardwareMonitor:
    """ç¡¬ä»¶ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self):
        self.gpu_available = self._check_gpu_availability()
    
    def _check_gpu_availability(self):
        """æ£€æŸ¥GPUå¯ç”¨æ€§"""
        try:
            gpus = GPUtil.getGPUs()
            return len(gpus) > 0
        except:
            return False
    
    def get_system_metrics(self):
        """è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'timestamp': datetime.now().isoformat()
        }
        
        # GPUä¿¡æ¯
        if self.gpu_available:
            try:
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu = gpus[0]  # è·å–ç¬¬ä¸€ä¸ªGPU
                    metrics.update({
                        'gpu_name': gpu.name,
                        'gpu_load': gpu.load * 100,
                        'gpu_memory_util': gpu.memoryUtil * 100,
                        'gpu_temperature': gpu.temperature
                    })
            except Exception as e:
                metrics['gpu_error'] = str(e)
        
        return metrics
    
    def diagnose_performance_issues(self, metrics):
        """è¯Šæ–­æ€§èƒ½é—®é¢˜"""
        issues = []
        
        if metrics['cpu_percent'] > 80:
            issues.append("CPUä½¿ç”¨ç‡è¿‡é«˜")
        
        if metrics['memory_percent'] > 85:
            issues.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜")
        
        if metrics.get('gpu_load', 0) > 90:
            issues.append("GPUè´Ÿè½½è¿‡é«˜")
        
        if metrics['disk_usage'] > 90:
            issues.append("ç£ç›˜ç©ºé—´ä¸è¶³")
        
        return issues

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´æµç¨‹"""
    print("ğŸš€ æ¼«å‰§è¡Œä¸šæ™ºèƒ½çˆ¬è™«ç³»ç»Ÿå¯åŠ¨")
    print("=" * 50)
    
    # 1. ç¡¬ä»¶ç›‘æ§
    monitor = HardwareMonitor()
    hardware_metrics = monitor.get_system_metrics()
    print("ğŸ–¥ï¸ ç¡¬ä»¶çŠ¶æ€:")
    print(f"   CPUä½¿ç”¨ç‡: {hardware_metrics['cpu_percent']}%")
    print(f"   å†…å­˜ä½¿ç”¨ç‡: {hardware_metrics['memory_percent']}%")
    if 'gpu_name' in hardware_metrics:
        print(f"   GPU: {hardware_metrics['gpu_name']} (è´Ÿè½½: {hardware_metrics['gpu_load']:.1f}%)")
    
    # 2. è¡Œä¸šç›®æ ‡å‘ç°
    crawler = MangaIndustryCrawler()
    targets = crawler.discover_industry_targets("æ¼«å‰§")
    print(f"\nğŸ¯ å‘ç° {len(targets)} ä¸ªç¬¦åˆæ¡ä»¶çš„ç›®æ ‡ç½‘ç«™")
    
    # 3. çˆ¬å–å†…å®¹
    if targets:
        manga_data = crawler.crawl_manga_content(targets)
        print(f"\nğŸ“š æˆåŠŸçˆ¬å– {len(manga_data)} éƒ¨æ¼«å‰§ä¿¡æ¯")
        
        # æ˜¾ç¤ºéƒ¨åˆ†ç»“æœ
        print("\nğŸ”¥ çƒ­é—¨æ¼«å‰§é¢„è§ˆ:")
        for i, manga in enumerate(manga_data[:5]):
            print(f"   {i+1}. {manga['title']} [{manga['category']}] - çƒ­åº¦: {manga['popularity_score']:.1f}")
    
    # 4. æ€§èƒ½è¯Šæ–­
    issues = monitor.diagnose_performance_issues(hardware_metrics)
    if issues:
        print(f"\nâš ï¸ æ€§èƒ½è­¦å‘Š: {', '.join(issues)}")
    else:
        print("\nâœ… ç³»ç»Ÿè¿è¡Œæ­£å¸¸")

if __name__ == "__main__":
    main()